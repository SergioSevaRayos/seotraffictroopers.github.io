<!DOCTYPE html>
<html lang="es">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <title>Creador de robots.txt - Herramientas SEO</title>

    <meta name="keywords"
        content="auditoria seo, auditoria web, seo tecnico, seo on page, seo off page, optimizacion web, posicionamiento web, consultoria seo, analisis seo, informe seo, mejorar seo, ranking google, palabras clave, tráfico web, marketing digital, tu negocio, Alicante">

    <meta name="description"
        content="Genera tu archivo robots.txt personalizado de forma rápida y sencilla con nuestra herramienta gratuita.">
    <link rel="canonical" href="https://tu-web.com/creador-robots.html">

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="icon" href="media/img/fabicon.webp" type="image/webp">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Karla:ital,wght@0,200..800;1,200..800&display=swap"
        rel="stylesheet">

    <link rel="stylesheet" href="style.css">

    <script type="application/ld+json">
        {
          "@context": "https://schema.org",
          "@type": "WebPage",
          "name": "Generador de robots.txt - Herramientas SEO",
          "description": "Crea tu archivo robots.txt personalizado de forma rápida y sencilla con nuestra herramienta gratuita.",
          "url": "https://seotraffictroopers.com/creador-robots.html", // URL específica de la página
          "image": "https://seotraffictroopers.com/media/img/robots-txt.webp", // Imagen relacionada con la herramienta
          "mainEntity": {
            "@type": "SoftwareApplication",
            "name": "Generador de robots.txt",
            "description": "Herramienta online para crear archivos robots.txt personalizados.",
            "url": "https://seotraffictroopers.com/creador-robots.html", // URL de la herramienta
            "operatingSystem": "Web Browser", // Opcional: especificar el sistema operativo
            "applicationCategory": "SEO", // Opcional: categoría de la aplicación
            "offers": { // Opcional: si ofreces la herramienta como parte de un paquete o servicio
              "@type": "Offer",
              "price": "0", // o "Consultar" si es parte de un servicio
              "priceCurrency": "EUR"
            }
          }
        }
        </script>
    <link rel="sitemap" type="application/xml" title="Sitemap" href="https://seotraffictroopers.com/sitemap.xml">
</head>

<body>
    <div class="contenedor-principal">
        <header>
            <nav>
                <ul class="navegacion">
                    <li><a href="index.html">Inicio</a></li>
                    <li class="dropdown">
                        <a href="#">Servicios</a>
                        <ul class="dropdown-content">
                            <li><a href="servicios.html">Todo</a></li>
                            <li><a href="seo-basico.html">Auditoría SEO Básica</a></li>
                            <li><a href="seo-completo.html">Auditoría SEO Completa</a></li>
                            <li><a href="seo-basico-mensual.html">Mantenimiento SEO Básico Mensual</a></li>
                            <li><a href="seo-avanzado-mensual.html">Mantenimiento SEO Avanzado Mensual</a></li>
                        </ul>
                    </li>
                    <li class="dropdown">
                        <a href="#">Herramientas</a>
                        <ul class="dropdown-content">
                            <li><a href="herramientas.html">Todo</a></li>
                            <li><a href="creador-robots.html">Generador de robots.txt</a></li>
                            <li><a href="creador-sitemap.html">Generador de sitemap.xml</a></li>
                            <li><a href="creador-metaetiquetas.html">Generador de metaetiquetas</a></li>
                        </ul>
                    </li>
                    <li><a href="quienes-somos.html">¿Quién soy?</a></li>
                    <li><a href="blog.html">Blog</a></li>
                    <li><a href="contacto.html">Contacto</a></li>
                </ul>
            </nav>
            <div class="header-content">
                <div class="header-left">
                    <img src="media/img/sergio2.webp" alt="Sergio Seva Rayos - Auditor SEO" class="SEO">
                </div>
                <div class="header-right">
                    <h2>Auditorías SEO</h2>
                    <a href="servicios.html"><button type="button">¿Quieres saber más?</button></a>
                </div>
            </div>
        </header>

        <main>
            <a href="https://wa.me/34633841203?text=Hola,%20me%20interesa%20tu%20servicio" class="whatsapp-float"
                target="_blank">
                <img src="media/img/logowhatsapp.webp" alt="Contactar por WhatsApp">
            </a>

            <section class="hero-arriba">
                <h1>Auditoría SEO Profesional para Mejorar tu Posicionamiento en Google</h1>
                <p>Descubre el potencial de tu sitio web y alcanza tus objetivos de negocio.</p>
                <a href="contacto.html" class="cta">¡Solicita tu Auditoría</a>
            </section>


            <section class="herramientas">

                <section class="generador-robots">
                    <h3>Generador de robots.txt</h3>
                    <p>Crea tu archivo robots.txt personalizado para controlar el acceso de los motores de búsqueda a tu
                        sitio web.</p>
                    <p>
                        El archivo robots.txt es una herramienta fundamental para controlar cómo los motores de búsqueda
                        interactúan con tu sitio web. Una configuración adecuada te permite indicar qué páginas quieres
                        que se indexen y cuáles no, optimizando así el rastreo y la indexación de tu contenido más
                        importante. Esto no solo mejora el SEO, sino que también ayuda a proteger información sensible y
                        a gestionar el presupuesto de rastreo de tu sitio de manera eficiente.
                    </p>

                    <p><strong>Instrucciones:</strong>
                    <ol>
                        <li>Introduce el User-agent (ej: *, Googlebot, Bingbot).</li>
                        <li>Introduce las URLs que quieres que sean indexadas, una por línea.</li>
                        <li>Introduce las URLs que no quieres que sean indexadas, una por línea.</li>
                        <li>Pulsa el botón "Generar robots.txt".</li>
                    </ol>
                    <p><strong>Ejemplo:</strong></p>
                    <p>Si quieres que todas las URLs de tu sitio web sean indexadas excepto las que están en el
                        directorio /privado/, introduce lo siguiente:</p>
                    <ul>
                        <li>User-agent: *</li>
                        <li>URLs indexables: /</li>
                        <li>URLs no indexables: /privado/</li>
                    </ul>
                    </p>

                    <form id="robots-form">
                        <div class="seccion">
                            <label for="user-agent">User-agent:</label>
                            <input type="text" id="user-agent" name="user-agent" value="*"
                                placeholder="Especifica el User-agent (ej: *, Googlebot)" required>
                        </div>
                        <div class="seccion">
                            <label for="urls-indexables">URLs indexables:</label>
                            <textarea id="urls-indexables"
                                placeholder="Introduce los html que quieres que sean indexables, una por línea. Ej: /index.html"></textarea>
                        </div>
                        <div class="seccion">
                            <label for="urls-no-indexables">URLs no indexables:</label>
                            <textarea id="urls-no-indexables"
                                placeholder="Introduce los html que quieres que no sean indexables, una por línea. Ej: /contacto.html"></textarea>
                        </div>

                        <button type="submit">Generar robots.txt</button>
                    </form>

                    <textarea id="robots-output" readonly></textarea>
                    <a id="download-link" href="#" download="robots.txt">Descargar robots.txt</a>
                </section>
                <a href="herramientas.html" class="cta">Atrás</a>
            </section>
            <section class="hero">
                <h2>Impulsa tu Negocio con una Auditoría SEO Profesional</h2>
                <a href="contacto.html" class="cta">¡Solicita tu Auditoría!</a>
                <p>Descubre el potencial de tu sitio web y alcanza tus objetivos de negocio.</p>
            </section>


            <script>
                const form = document.getElementById('robots-form');
                const userAgentInput = document.getElementById('user-agent');
                const urlsIndexablesInput = document.getElementById('urls-indexables');
                const urlsNoIndexablesInput = document.getElementById('urls-no-indexables');
                const robotsOutput = document.getElementById('robots-output');
                const downloadLink = document.getElementById('download-link');

                form.addEventListener('submit', (event) => {
                    event.preventDefault();

                    const userAgent = userAgentInput.value;
                    const urlsIndexables = urlsIndexablesInput.value.split('\n').filter(url => url.trim() !== '');
                    const urlsNoIndexables = urlsNoIndexablesInput.value.split('\n').filter(url => url.trim() !== '');

                    let robotsContent = `User-agent: ${userAgent}\n`; // Incluir User-agent
                    urlsIndexables.forEach(url => robotsContent += `Allow: ${url}\n`);
                    urlsNoIndexables.forEach(url => robotsContent += `Disallow: ${url}\n`);

                    robotsOutput.value = robotsContent;

                    const blob = new Blob([robotsContent], { type: 'text/plain' });
                    const url = URL.createObjectURL(blob);
                    downloadLink.href = url;
                    downloadLink.style.display = 'block';
                });
            </script>
        </main>

        <footer>
            <div class="footer-content">
                <div class="footer-left">
                    <p>&copy; <span id="currentYear">2024</span> SEO Traffic Troopers</p>
                </div>
                <div class="footer-logo">
                    <img src="media/img/logotipostt.webp" alt="SEO Traffic Troopers Logo">
                </div>
                <div class="footer-right">
                    <nav>
                        <ul class="footer-links">
                            <li><a href="aviso-legal.html">Aviso Legal</a></li>
                            <li><a href="politica-privacidad.html">Política de Privacidad</a></li>
                            <li><a href="politica-cookies.html">Política de Cookies</a></li>
                            <li><a href="mapa-del-sitio.html">Mapa del Sitio</a></li>
                        </ul>
                    </nav>
                </div>
            </div>
        </footer>
    </div>
    <script src="script.js"></script>
</body>

</html>